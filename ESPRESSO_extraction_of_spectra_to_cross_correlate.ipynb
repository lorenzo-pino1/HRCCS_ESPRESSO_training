{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal of this workshop**: \n",
    "\n",
    "Get acquainted with reduction of optical data, with a particular focus on ESPRESSO and on the differences with nIR data. The following notebook is fully functional, provided that you have: (1) python 3 and some very common packages that can be installed with anaconda; (2) some ESPRESSO data already processed through the ESO pipelines.\n",
    "\n",
    "For the latter, I give you all the steps that you can apply yourself to any set of public or proprietary data to which you have access. This is a very disk space and time consuming process, thus I do not provide the reduced data here.\n",
    "\n",
    "You will get this notebook and, on request and on collaborative basis, the full pipeline that I am writing for this reduction. This is a highly modular pipeline (to complement it with stuff like PCA and other methods) and compatible with ESPRESSO, HARPS-N and MAROON-X. I plan to make it public, it is currently in development but is already functional (with this same code, I have published my Pino et al., 2020 paper).\n",
    "\n",
    "Write me to lorenzo.pino@inaf.it if interested!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESO pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike with MAROON-X, we do not have an efficient team of people giving us the data in a usable form. We have to prepare our own data, using ESO pipelines.\n",
    "\n",
    "ESO pipelines are a bit messy to install, but the ESO support team is very responsive and will help you in the process. I managed to install the pipelines on a Mac relatively effortlessly.\n",
    "\n",
    "ESO pipelines are called \"recipes\", and can be run in different environments. Esorex is a command line tool. However, there is a convenient API called Esoreflex, which wraps Esorex in a more immediate form. This is what I use for the ESPRESSO pipelines.\n",
    "\n",
    "From the README: Data reduction is demanding in terms of system resources. The user should have at least 16 GB of RAM, and 1 TB of free disk space. Better is to have 32 GB of RAM and 4 TB of free disk space. Please note that only the demo requires 30 GB of free space to be installed and generates additional 10 GB of products. This sets also minimum root directory space requirements for RPM installation. Data reduction for SINGLE modes (in particular not binned data) is slow, and may take 45 min. As some recipes parallelise code using OpenMP, data reduction is faster on multi-core systems.\n",
    "\n",
    "... Yes, it is kind of a nightmare.\n",
    "\n",
    "Create a folder tree:\n",
    "ESPRESSO_observations/<Run_name>/data_with_raw_calibs\n",
    "ESPRESSO_observations/<Run_name>/reflex_end_products\n",
    "\n",
    "We will have two runs: WASP-76b_transmission_run_1; WASP-76b_transmission_run_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will use the data published in Ehrenreich et al. (2021). Step 0 is to get the data from the ESO archive. \n",
    "\n",
    "There is a user friendly version of the archive (science portal) that contains pre-reduced spectra. These are good for a quick look, but not for science. So, we instead use the raw data portal (http://archive.eso.org/eso/eso_archive_main.html)\n",
    "\n",
    "Let's search for WASP-76b, and select the ESPRESSO instrument. To make sure you do not lose any data, click on the Program ID. Ehrenreich et al. uses two runs, both public: 1102.C-0744(C) and 1102.C-0744(D). In the window that opens, click on \"FileList\". That gives you a list of all science exposures in that run. We \"Mark all\", and \"Request marked datasets (new service)\".\n",
    "\n",
    "We will not just need the science exposures, but also the calibrations, in order to run the pipeline. So let's \"run association\" of the **raw** calibrations (not processed). After the associatiton is done (it takes a while), we can download everything. I recommend the shell script.\n",
    "\n",
    "Place the script for each run in the \"data_with_raw_calibs\", change permissions if needed (chmod u+x <script_name>) and run. You are downloading about 10 GB of data, so it may take a while. After everything is done, unzip the archives.\n",
    "\n",
    "You should have lots of fits files. Most of them are calibrations, and may contain a date that does not coincide with the observational night. The observations have an associated raw2raw.xml, which associates the calibrations to the exposure. If some files are missing, repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using esoreflex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need a configuration file of type \".kar\". If you installed your ESO pipelines, you'll have an example to follow. In this drive (https://drive.google.com/drive/u/2/folders/1Xfu2wWyn8WO1DgOQsYmxj8gBMTMWUJ-W) I placed the one that I used for the tutorial.\n",
    "\n",
    "1) Change the RAW_DATA_DIR and END-PRODUCTS-DIR according to your tree;\n",
    "2) Tools -> Animate at Runtime -> 10\n",
    "\n",
    "Launch!\n",
    "\n",
    "Esoreflex requires some interaction. First, it will read the files, and present you with boxes to tick to select the files you'd like to reduce. If they are greyd out, it means that it could not find the calibrations. Go back to previous steps, and/or check your directory tree.\n",
    "\n",
    "In the first pop-up window, have a look at the extracted spectra using the mask. If this is satisfactory, click the option \"Use the parameters above as initial values in the subsequent executions of this receipe\". In this way you'll have no more pop-ups, and esoreflex will run on its own.\n",
    "\n",
    "You can also interrupt the execution of the pipeline and resume at a different moment. Esoreflex knows which files you have reduced or not, and by default it should not overwrite them. If you wish to overwrite, find the \"lazy mode\" and disable it (so Esoreflex will actually do everything from the beginning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of esoreflex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output folder, esoreflex creates a folder with the date and time on which the workflow was called. So if you do parts of the analysis on different times, you will have the results under different folders. Within this folder you will find one subfolder for each exposure, which contains the final products. Only some of them are useful for us.\n",
    "\n",
    "ESPRESSO has two fibers. Fiber A contains the spectra of the planet, and fiber B can be placed on different sources depending on the observer's choice. Usually, it is placed on sky, to monitor telluric emission.\n",
    "\n",
    "**<OB_name>_S2D_BLAZE_A_<Esposure_UT_time>**: gives the blaze function in fiber A.\n",
    "\n",
    "**<OB_name>_S2D_SKYSUB_A_<Esposure_UT_time>**: gives the 2-dimensional spectra from fiber A, after subtracting fiber B. This is necessary to remove the telluric emission lines, but it could in some cases introduce extra noise. This is what I am currently using and is suggested, but you may explore the impact of avoiding the sky subtrction.\n",
    "\n",
    "In addition, there are many other files, for both fiber A and B. For example, there are flux calibrated spectra (currently, Feb 2022, ESPRESSO flux calibration is not completely reliable, and not accurate enough to recover the flux at the precision needed for exoplanet atmosphere applications). Finally, there are CCFs calculated by the pipeline with an algorithm which is very similar to the one I give you (but much slower!).\n",
    "\n",
    "From here on, we are on our own and we can start the data reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to nIR data, optical data can be seen as \"easier\", in the sense that we have less telluric contamination. We have more contamination from the star, but if this is well behaved it is relatively simple to correct for (but this depends on application).\n",
    "\n",
    "I thus provide you with a very simple data reduction pipeline, which *does not correct for tellurics*, and is already sufficient to reproduce results from the literature (e.g., Ehrenreich et al., 2021; Pino et al., 2020). Of course, this can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the operations we need to perform:\n",
    "\n",
    "1) Read data;\n",
    "\n",
    "2) Apply color correction;\n",
    "\n",
    "3) Move the stellar rest frame to remove stellar lines;\n",
    "\n",
    "4) Additional flattening and filtering of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import libraries, and open the fits files. Some tidying now will spare headaches later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from astropy import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with fits files. Works with different kinds of inputs (SKYSUB or not etc).\n",
    "\n",
    "def read_sorted_hdus(input_data_directory, data_type, target_name):\n",
    "    \n",
    "    # Get all data in folder of the specified type\n",
    "    import glob\n",
    "    data_files = np.array(glob.glob(input_data_directory + '*_{}_*'.format(data_type)))\n",
    "    \n",
    "    # Read date and time from the filename (standardized)\n",
    "    date_time_observations = get_date_time_observations(data_files, data_type=data_type)\n",
    "    \n",
    "    # Sort exposures in time\n",
    "    time_sorted_exposures_mask = compute_time_sorted_exposures_mask(date_time_observations)\n",
    "    data_files_sorted = data_files[time_sorted_exposures_mask]\n",
    "\n",
    "    # Open the fits and return them. Not the cheapest in terms of memory, but we'll use some of the info in the headers\n",
    "    hdus = []\n",
    "    for file_name in data_files_sorted:\n",
    "        hdu = fits.open(file_name)\n",
    "        if hdu[0].header['HIERARCH ESO OBS TARG NAME'] == target_name:\n",
    "            hdus.append(hdu)    \n",
    "\n",
    "    return hdus\n",
    "\n",
    "def get_date_time_observations(data_files, data_type):\n",
    "    \"\"\" Extract the date and time of an expsure from its name.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    re.compile(data_type)\n",
    "\n",
    "    date_time_observations = [] \n",
    "\n",
    "    for file in data_files:\n",
    "        start = re.search(data_type + '_', file).end()\n",
    "\n",
    "        date = file[start:start+10]\n",
    "        time = file[start+11:start+23]\n",
    "\n",
    "        date_time_observations.append([date, time])\n",
    "\n",
    "    return date_time_observations\n",
    "\n",
    "def compute_time_sorted_exposures_mask(date_time_observations):\n",
    "    \"\"\" From strings to datetime objects\n",
    "    \"\"\"\n",
    "\n",
    "    date_time_observations_datetime=[]\n",
    "    from datetime import datetime\n",
    "    for dt in date_time_observations:\n",
    "        date_time_observations_datetime.append(datetime.strptime(dt[0] + ' ' + dt[1][:-4], \"%Y-%m-%d %H:%M:%S\"))    \n",
    "\n",
    "    return np.argsort(date_time_observations_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_directory = '/data2/pino/Data/observations/ESPRESSO/W76b_2018_10_31/'\n",
    "data_type='S2D_SKYSUB_A'\n",
    "target_name='BD+01   316' # From the header of ESPRESSO files\n",
    "\n",
    "# Read the sorted hdus\n",
    "hdus_S2D = read_sorted_hdus(input_data_directory = input_data_directory, data_type=data_type, target_name=target_name)\n",
    "\n",
    "len(hdus_S2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdus_S2D[0][0].header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the actual spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_S2D(hdus):\n",
    "    \"\"\"S2D hdu structure (works for any S2D file):\n",
    "\n",
    "    1: science\n",
    "    2: errors\n",
    "    3: data quality flag\n",
    "    4: wavelength vacuum\n",
    "    5: wavelength air\n",
    "    6: pixel width vacuum\n",
    "    7: pixel width air\n",
    "\n",
    "    Output: (exposure, order, pixel)\n",
    "\n",
    "    Every physical order is represented twice in the S2D spectrum.\n",
    "\n",
    "    From the manual:\n",
    "    In the case of ESPRESSO, pupil slicing and anamorphic magnification are used to achieve high spectral resolution while using a grating with the same size as that of HARPS. The slit image on the detector is made of two ellipsoidal slices aligned in cross-dispersion direction. For the modes with the two slices of an order well separated on the CCD (SINGLEHR11, SINGLEHR21 and SINGLEUHR), each slice is extracted separately and treated as if it was an independent interference order. This results in each physical order being represented twice in the S2D extracted spectrum.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    S2D_wavelengths_vacuum_bary = []\n",
    "    S2D_spectra = []\n",
    "    S2D_errors = []\n",
    "\n",
    "    for n, hdu in enumerate(hdus):\n",
    "        try:\n",
    "            S2D_wavelengths_vacuum_bary.append(hdu[4].data)\n",
    "        except:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        S2D_spectra.append(hdu[1].data)\n",
    "        S2D_errors.append(hdu[2].data)\n",
    "\n",
    "    return np.array(S2D_wavelengths_vacuum_bary), np.array(S2D_spectra), np.array(S2D_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2D_wavelengths_vacuum_bary, S2D_spectra, S2D_errors = read_S2D(hdus_S2D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at the spectra. Since we will do that many times, let's define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (exposure, order, pixel)\n",
    "\n",
    "def plot_single_exposure(S2D_spectra, exp_index, cmap = matplotlib.cm.get_cmap('Blues')):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    norm = visualization.ImageNormalize(S2D_spectra[exp_index], interval=visualization.ZScaleInterval())\n",
    "    imshow = ax.imshow(S2D_spectra[exp_index], norm=norm, origin='lower', aspect='auto', cmap=cmap, interpolation='none')\n",
    "    ax.set_xlabel('Pixel')\n",
    "    ax.set_ylabel('order')\n",
    "    fig.colorbar(imshow)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def plot_single_order(S2D_spectra, order_index, cmap = matplotlib.cm.get_cmap('Blues')):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    norm = visualization.ImageNormalize(S2D_spectra[:,order_index], interval=visualization.ZScaleInterval())\n",
    "    imshow = ax.imshow(S2D_spectra[:,order_index], norm=norm, origin='lower', aspect='auto', cmap=cmap, interpolation='none')\n",
    "    ax.set_xlabel('Pixel')\n",
    "    ax.set_ylabel('Exposure')\n",
    "    fig.colorbar(imshow)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_exp, ax_exp = plot_single_exposure(S2D_spectra, exp_index=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 2 CCDs have different useful pixel numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_order, ax_order = plot_single_order(S2D_spectra, order_index=130)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,s in zip(S2D_wavelengths_vacuum_bary[10], S2D_spectra[10]):\n",
    "    plt.plot(w, s)\n",
    "    \n",
    "plt.ylim(0, 6000)\n",
    "plt.xlim(4000, 4200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the shape of the spectra consistent throughout the orders. This is not super-critical, and for the purpose of this tutorial I leave it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of stellar lines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a common wavelength grid which on which we will provide the final spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read BERV\n",
    "#\n",
    "\n",
    "BERVs = []\n",
    "for hdu in hdus_S2D:\n",
    "    BERVs.append(hdu[0].header['ESO QC BERV'])\n",
    "\n",
    "#\n",
    "# Calculate the laboratory vacuum wavelength solution, which we will use as a fixed, common grid throughout the analysis.\n",
    "#\n",
    "\n",
    "S2D_wavelengths_vacuum_laboratory = []\n",
    "\n",
    "# The pipeline provides the wavelength array already shifted in the barycentric rest frame, so we shift it back.\n",
    "\n",
    "from astropy.constants import c\n",
    "c = (c.to('km s-1')).value\n",
    "\n",
    "for n in range(len(S2D_wavelengths_vacuum_bary)):\n",
    "    S2D_wavelengths_vacuum_laboratory.append(S2D_wavelengths_vacuum_bary[n] - S2D_wavelengths_vacuum_bary[n]*BERVs[n]/c)\n",
    "S2D_wavelengths_vacuum_laboratory = np.array(S2D_wavelengths_vacuum_laboratory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to consider that the star is moving around the center of mass of the system. Thus, rather than calculating the spectra in the barycentric rest frame, we need to move to a system where the star is stationary.\n",
    "\n",
    "To do that, we need to know which planetary phases we are sampling, and calculate the corresponding stellar shift. In the process, we also calculate and store the planet phases and the planet velocitie which we will use in the following.\n",
    "\n",
    "We need to take care of the time conversions: the known ephemerid is given in BJD TDB, so we need to convert our exposure times to that same time scale and format. We use astropy for that. To do so, we calculate the BJD TDB time of every exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import time\n",
    "from astropy.coordinates import SkyCoord, EarthLocation\n",
    "\n",
    "target_coordinates = SkyCoord.from_name(target_name)\n",
    "site = EarthLocation.of_site('paranal')\n",
    "\n",
    "time_observations = []\n",
    "\n",
    "for hdu in hdus_S2D:\n",
    "    exp_start=hdu[0].header['MJD-OBS']\n",
    "    exp_dur=hdu[0].header['EXPTIME']\n",
    "    time_observations.append(exp_start + exp_dur/(60.*60.*24.))\n",
    "\n",
    "time_observations=time.Time(np.array(time_observations), format='mjd', scale='utc', location=site)\n",
    "\n",
    "# We compute the time travel difference to the barycenter of the Solar System\n",
    "dt_bary = time_observations.light_travel_time(target_coordinates)\n",
    "\n",
    "# For the barycentric times, which are of our interest, the correct scale is TDB. (https://docs.astropy.org/en/stable/time/index.html#barycentric-and-heliocentric-light-travel-time-corrections).\n",
    "bjd_time_observations = time_observations.tdb + dt_bary  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate the planet phases, the planet velocities and the stellar velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 =   2457273.4191 # BJD TDB; https://ui.adsabs.harvard.edu/abs/2022ApJS..258...40K/abstract\n",
    "P_days = 1.8098806\n",
    "r_AU = 0.0330\n",
    "\n",
    "phases = ((bjd_time_observations.jd-T0)/P_days)%1\n",
    "phases = np.where(phases < 0.5, 1+phases, phases) # To have them as a monotic sequence\n",
    "\n",
    "r_km = r_AU*1.496e8\n",
    "P_sec = P_days*24.*60.*60.\n",
    "v_max = 2*np.pi*r_km/P_sec\n",
    "planetary_velocities_during_transit = v_max*np.sin(phases*2.*np.pi)\n",
    "\n",
    "k_star = 0.1193 # km/s\n",
    "stellar_phases = 0.5+phases # Change the sign of the RV for the star.\n",
    "stellar_velocities_during_transit = k_star*np.sin(stellar_phases*2.*np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the shift in wavelength in a rest frame which is stationary with respect to the star.\n",
    "$$\\lambda_\\mathrm{emit} = \\lambda_\\mathrm{obs} / (1 + v_\\mathrm{emit}/c)\\sim\\lambda_\\mathrm{obs}\\cdot (1 - v_\\mathrm{emit}/c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2D_wavelengths_vacuum_bary_stationary_star = []\n",
    "\n",
    "for n in range(len(S2D_wavelengths_vacuum_bary)):\n",
    "    S2D_wavelengths_vacuum_bary_stationary_star.append(S2D_wavelengths_vacuum_bary[n] - S2D_wavelengths_vacuum_bary[n]*stellar_velocities_during_transit[n]/c)\n",
    "S2D_wavelengths_vacuum_bary_stationary_star = np.array(S2D_wavelengths_vacuum_bary_stationary_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move the spectra to the reference frame which co-moves with the star. The star does not move in this reference frame, thus it's best removed here.\n",
    "\n",
    "Practically, we express these shifted spectra onto the commong wavelength grid (S2D_wavelengths_vacuum_laboratory), to be able to work with them as a time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2D_spectra_stellar_rest_frame = []\n",
    "for n in range(len(S2D_spectra)):\n",
    "    S2D_spectra_stellar_rest_frame.append([np.interp(w, wn, s) for w, wn, s in zip(S2D_wavelengths_vacuum_laboratory[n], S2D_wavelengths_vacuum_bary_stationary_star[n], S2D_spectra[n])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that adjacent orders do not \"match\". That is becasue the units we have been working on is \"total photoelectrons received during the exposure time\". In this specific pipeline we could work in photoelectrons, but I think for some cases it may be better to work in units of energy which is also more closely related to flux (flux = energy per unit time, area, solid angle). So let's calcuate howmuch energy has been deposited in every single pixel.\n",
    "\n",
    "For each pixel, we have to multiply by the total number of photons that hit it by the energy carreid by that photon. This is $h\\nu$ or $\\hbar/\\lambda$. We neglect the constants, because we will normalize everything anyways. Note that we are assuming a single energy for all photoeleectrons falling on one pixel. You can verify yourself that this is a good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_bins = []\n",
    "\n",
    "for n,w in enumerate(S2D_wavelengths_vacuum_laboratory[0]):\n",
    "    \n",
    "    #Compute the symmetric bins that cover the entire axis of wavelegths for a given set of wavelengths where the spectrum is known.\n",
    "    #First bin extrapolated as identical to the second, and last bin to the previous.\n",
    "    \n",
    "    wave_bins.append([])\n",
    "    wave_bins[n] = [ [(w[i-1] + w[i])/2.,(w[i] + w[i+1])/2.] for i in range(1,len(w)-1)]\n",
    "    wave_bins[n].insert(0, [w[0] - (w[1] - w[0])/2., w[0] + (w[1] - w[0])/2. ])\n",
    "    wave_bins[n].append([w[-1] - (w[-1] - w[-2])/2., w[-1] + (w[-1] - w[-2])/2. ])\n",
    "\n",
    "wave_bins = np.array(wave_bins)\n",
    "    \n",
    "S2D_spectra_stellar_rest_frame = S2D_spectra_stellar_rest_frame/np.diff(wave_bins, axis=2)[:,:,0][None,:] # Now this is unit of total energy deposited in the pixel during the exposure time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now verify that the downwar slope is gone, and each order naturally goes into the next one. Spectral lines now have the correct shape. The \"jumps\" you can still see are not real, and are instead due to the high noise found in the blue part of each order because of the blaze function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,s in zip(S2D_wavelengths_vacuum_laboratory[10], S2D_spectra_stellar_rest_frame[10]):\n",
    "    plt.plot(w, s)\n",
    "    \n",
    "np.save('S2D_spectra_stellar_rest_frame_with_star_included', S2D_spectra_stellar_rest_frame)\n",
    "    \n",
    "plt.ylim(1.e3, 5.e5)\n",
    "plt.xlim(4000, 4200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spectra are finally ready to remove the stellar lines. To do that, we use the same trick employed by PCA. Since telluric correction is much less important in the optical, we ignore that, and we can use a much simpler method. We simply build a master spectrum by taking a median in time of all the spectra, while we are in the star rest frame. The planet is moving, and it is not removed. Tellurics are not removed because they vary in time.\n",
    "\n",
    "However, right now, the trick would not work because we have variable throughput during the night, and the median would give nonsense results. So let's first bring all the spectra to a common level. We do this order by order, because we did not perform a color-correction. I do this order by order also when I do perform the color-correction, because I think this is the most reliable way (color-correction still corrects inter-order continuum variations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the two central quartes of each order\n",
    "i1 = 2277\n",
    "i2=6833\n",
    "\n",
    "S2D_spectra_stellar_rest_frame_rescaled = []\n",
    "for s in S2D_spectra_stellar_rest_frame:\n",
    "    S2D_spectra_stellar_rest_frame_rescaled.append(s/np.average(s[:,i1:i2], axis=1)[:,None])\n",
    "S2D_spectra_stellar_rest_frame_rescaled = np.array(S2D_spectra_stellar_rest_frame_rescaled)\n",
    "\n",
    "    \n",
    "print(np.shape(S2D_spectra_stellar_rest_frame))\n",
    "print(np.shape(S2D_spectra_stellar_rest_frame_rescaled))\n",
    "    \n",
    "fig_order, ax_order = plot_single_order(S2D_spectra_stellar_rest_frame, order_index=130)\n",
    "fig_order_rescaled, ax_order_rescaled = plot_single_order(S2D_spectra_stellar_rest_frame_rescaled, order_index=130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the spectra are **rescaled** we can  **normalize** them by a master stellar spectrum (which contains some tellurics as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2D_spectra_stellar_rest_frame_normalized = []\n",
    "median_S2D_spectra_stellar_rest_frame_rescaled = np.median(S2D_spectra_stellar_rest_frame_rescaled, axis=0)\n",
    "for s in S2D_spectra_stellar_rest_frame_rescaled:\n",
    "    S2D_spectra_stellar_rest_frame_normalized.append(s/median_S2D_spectra_stellar_rest_frame_rescaled)\n",
    "S2D_spectra_stellar_rest_frame_normalized = np.array(S2D_spectra_stellar_rest_frame_normalized)\n",
    "\n",
    "fig_order_normalized, ax_order_normalized = plot_single_order(S2D_spectra_stellar_rest_frame_normalized, order_index=131)\n",
    "\n",
    "# Plot master ontop of one exposure\n",
    "plt.figure()\n",
    "for o in range(170):\n",
    "    plt.plot(S2D_wavelengths_vacuum_laboratory[50, o], S2D_spectra_stellar_rest_frame_rescaled[50, o], 'k', S2D_wavelengths_vacuum_laboratory[50, o], median_S2D_spectra_stellar_rest_frame_rescaled[o], 'r')\n",
    "\n",
    "plt.xlim(4100, 4110)\n",
    "plt.ylim(-0.1, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the first orders have less pixels? When we divide by zero we get a warning. If we are not extremely careful, our cross-correlations and likelihoods will make no sense. I thus prefer to set nans to zeros, which is the null value for the binary mask cross-correlations (if you use other methods, you may need to set it to one, or other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2D_spectra_stellar_rest_frame_normalized = np.nan_to_num(S2D_spectra_stellar_rest_frame_normalized)\n",
    "\n",
    "# Plot final spectra\n",
    "plt.figure()\n",
    "for o in range(170):\n",
    "    plt.plot(S2D_wavelengths_vacuum_laboratory[50, o], S2D_spectra_stellar_rest_frame_normalized[50, o], 'k')\n",
    "\n",
    "plt.xlim(4100, 4200)\n",
    "plt.ylim(-0.1, 1.5)\n",
    "\n",
    "np.save('S2D_wavelengths_vacuum_laboratory', S2D_wavelengths_vacuum_laboratory)\n",
    "np.save('S2D_spectra_barycentric_normalized', S2D_spectra_stellar_rest_frame_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to cross-correlate! Of course, you should be aware that we have not corrected for tellurics. While this can be fine for the bluest orders, for example if you focus on neutral iron, or it could be fine for a quick look, you probably want to implement a telluric correction or at least mask out the contaminated regions before proceeding.\n",
    "\n",
    "Also, note how the singal to noise is strongly varying with wavelength. This is something that we need to consider when cross-correlating with yout favorite method. Did you already guess why the noise in the blue parts of the order is so high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "data_files_A_blaze = glob.glob(input_data_directory + '*BLAZE_A_*')\n",
    "blaze_profiles = fits.open(data_files_A_blaze[0])[1].data\n",
    "\n",
    "for w, b in zip(S2D_wavelengths_vacuum_laboratory[0], blaze_profiles):\n",
    "    plt.plot(w, b, 'k')\n",
    "    \n",
    "plt.xlim(4100, 4200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, two more operations can be performed on data, which will improve the significance. ESPRESSO is a good instruments, thus the improvement is only marginal. I do not include them here. In the full pipeline, I will implement those methods.\n",
    "1. High-pass filtering to remove smooth variations (imperfect blaze correction or color-correction, ...)\n",
    "2. Removal of outliers (cosmic rays etc).\n",
    "In ESPRESSO, the first step is important for publishing since there is an effect called \"wiggles\", which are low frequency oscillations observed ontop of the specrta, with phase change typically occurring at half of the night."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
